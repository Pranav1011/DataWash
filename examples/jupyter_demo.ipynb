{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataWash Jupyter Demo\n",
    "\n",
    "This notebook demonstrates DataWash's features in a Jupyter environment:\n",
    "- Rich HTML rendering of reports\n",
    "- Interactive exploration of suggestions\n",
    "- Step-by-step cleaning with selective application\n",
    "- Before/after quality score visualization\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install datawash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datawash import analyze\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"DataWash loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Sample Messy Data\n",
    "\n",
    "Let's create a dataset with various quality issues that DataWash can detect and fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create messy sample data with multiple issues\n",
    "df = pd.DataFrame({\n",
    "    \"customer_name\": [\"John Smith\", \"JANE DOE\", \"bob wilson\", \"  Alice Brown  \", \"Charlie Davis\",\n",
    "                      \"Diana Miller\", \"EDWARD JONES\", \"fiona garcia\", \"George Lee\", \"Hannah White\"],\n",
    "    \"email\": [\"john@email.com\", \"\", \"bob@email.com\", \"alice@email.com\", None,\n",
    "              \"diana@email.com\", \"  edward@email.com  \", \"\", \"george@email.com\", \"hannah@email.com\"],\n",
    "    \"age\": [\"28\", \"34\", \"45\", \"29\", \"38\", \"42\", \"31\", \"27\", \"35\", \"40\"],  # Stored as strings!\n",
    "    \"purchase_amount\": [150.00, 230.50, 89.99, 1250.00, 175.25, 95.00, 310.00, 88.50, 450.00, 125.75],\n",
    "    \"is_premium\": [\"yes\", \"Yes\", \"YES\", \"no\", \"No\", \"NO\", \"true\", \"True\", \"false\", \"False\"],\n",
    "    \"signup_date\": [\"2023-01-15\", \"15/02/2023\", \"March 10, 2023\", \"2023-04-20\", \"2023/05/25\",\n",
    "                    \"25-Jun-2023\", \"July 4, 2023\", \"2023-08-15\", \"15/09/2023\", \"October 1, 2023\"],\n",
    "})\n",
    "\n",
    "# Add a duplicate row\n",
    "df = pd.concat([df, df.iloc[[0]]], ignore_index=True)\n",
    "\n",
    "print(f\"Created dataset with {len(df)} rows and {len(df.columns)} columns\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze the Data\n",
    "\n",
    "Use DataWash to analyze the dataset. In Jupyter, the report renders as a rich HTML table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the data\n",
    "report = analyze(df)\n",
    "\n",
    "# In Jupyter, this automatically renders as HTML!\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore the Quality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data Quality Score: {report.quality_score}/100\")\n",
    "print(f\"\\nIssues Found: {len(report.issues)}\")\n",
    "print(f\"Suggestions: {len(report.suggestions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. View Detailed Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explore Issues in Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of issues for easy exploration\n",
    "issues_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Type\": issue.issue_type,\n",
    "        \"Severity\": issue.severity.value,\n",
    "        \"Columns\": \", \".join(issue.columns) if issue.columns else \"all\",\n",
    "        \"Message\": issue.message[:50] + \"...\" if len(issue.message) > 50 else issue.message,\n",
    "        \"Confidence\": f\"{issue.confidence:.0%}\"\n",
    "    }\n",
    "    for issue in report.issues\n",
    "])\n",
    "\n",
    "issues_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explore Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of suggestions\n",
    "suggestions_df = pd.DataFrame([\n",
    "    {\n",
    "        \"ID\": s.id,\n",
    "        \"Priority\": s.priority.value,\n",
    "        \"Action\": s.action,\n",
    "        \"Transformer\": s.transformer,\n",
    "        \"Impact\": s.impact\n",
    "    }\n",
    "    for s in report.suggestions\n",
    "])\n",
    "\n",
    "suggestions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Apply Specific Suggestions\n",
    "\n",
    "Instead of applying all suggestions, let's selectively apply some using their IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply only the first 3 suggestions\n",
    "ids_to_apply = [s.id for s in report.suggestions[:3]]\n",
    "print(f\"Applying suggestions: {ids_to_apply}\")\n",
    "\n",
    "for s in report.suggestions[:3]:\n",
    "    print(f\"  [{s.id}] {s.action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply selected suggestions\n",
    "partial_clean_df = report.apply(ids_to_apply)\n",
    "\n",
    "print(f\"\\nApplied {len(ids_to_apply)} transformations\")\n",
    "partial_clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Apply All Remaining Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's apply ALL suggestions for comparison\n",
    "# First, re-analyze to get fresh report\n",
    "report = analyze(df)\n",
    "clean_df = report.apply_all()\n",
    "\n",
    "print(f\"Applied all {len(report.suggestions)} transformations\")\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Before/After Quality Score Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get quality scores\n",
    "score_before = report._last_score_before\n",
    "score_after = report._last_score_after\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(['Before', 'After'], [score_before, score_after], \n",
    "               color=['#ff6b6b', '#51cf66'], edgecolor='black')\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.set_ylabel('Quality Score')\n",
    "ax1.set_title('Data Quality Score: Before vs After')\n",
    "ax1.axhline(y=80, color='green', linestyle='--', alpha=0.5, label='Good threshold')\n",
    "ax1.axhline(y=60, color='orange', linestyle='--', alpha=0.5, label='Warning threshold')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, [score_before, score_after]):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
    "             f'{score}', ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Improvement gauge\n",
    "ax2 = axes[1]\n",
    "improvement = score_after - score_before\n",
    "ax2.pie([improvement, 100-improvement], \n",
    "        labels=[f'+{improvement} points', ''], \n",
    "        colors=['#51cf66', '#e9ecef'],\n",
    "        startangle=90,\n",
    "        explode=[0.05, 0])\n",
    "ax2.set_title(f'Quality Improvement: +{improvement} points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nImprovement Summary:\")\n",
    "print(f\"  Before: {score_before}/100\")\n",
    "print(f\"  After:  {score_after}/100\")\n",
    "print(f\"  Change: +{improvement} points ({improvement/score_before*100:.1f}% improvement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compare Original vs Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ORIGINAL DATA:\")\n",
    "print(f\"Rows: {len(df)}, Columns: {len(df.columns)}\")\n",
    "print(f\"Data types: {dict(df.dtypes)}\")\n",
    "print()\n",
    "\n",
    "print(\"CLEANED DATA:\")\n",
    "print(f\"Rows: {len(clean_df)}, Columns: {len(clean_df.columns)}\")\n",
    "print(f\"Data types: {dict(clean_df.dtypes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison of specific columns\n",
    "comparison_cols = ['customer_name', 'is_premium', 'signup_date']\n",
    "\n",
    "print(\"Before cleaning:\")\n",
    "display(df[comparison_cols].head())\n",
    "\n",
    "print(\"\\nAfter cleaning:\")\n",
    "display(clean_df[comparison_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate Reproducible Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Python code that reproduces the cleaning\n",
    "code = report.generate_code(style=\"function\")\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Use Case Comparison: General vs ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare suggestions for different use cases\n",
    "report_general = analyze(df, use_case=\"general\")\n",
    "report_ml = analyze(df, use_case=\"ml\")\n",
    "\n",
    "print(\"GENERAL use case priorities:\")\n",
    "for s in report_general.suggestions[:5]:\n",
    "    print(f\"  [{s.priority.value:6}] {s.action}\")\n",
    "\n",
    "print(\"\\nML use case priorities:\")\n",
    "for s in report_ml.suggestions[:5]:\n",
    "    print(f\"  [{s.priority.value:6}] {s.action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Export Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data to CSV\n",
    "clean_df.to_csv(\"cleaned_data.csv\", index=False)\n",
    "print(\"Saved cleaned data to 'cleaned_data.csv'\")\n",
    "\n",
    "# You can also save to other formats\n",
    "# clean_df.to_parquet(\"cleaned_data.parquet\")  # Requires pyarrow\n",
    "# clean_df.to_excel(\"cleaned_data.xlsx\", index=False)  # Requires openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated:\n",
    "\n",
    "1. **Rich HTML rendering** - Reports display beautifully in Jupyter\n",
    "2. **Interactive exploration** - Easily explore issues and suggestions as DataFrames\n",
    "3. **Selective application** - Apply specific fixes using `report.apply([1, 2, 3])`\n",
    "4. **Quality visualization** - Track improvement with before/after scores\n",
    "5. **Code generation** - Get reproducible Python code\n",
    "6. **Use case comparison** - Different priorities for ML vs general cleaning\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try with your own data: `report = analyze(\"your_data.csv\")`\n",
    "- Experiment with different use cases: `analyze(df, use_case=\"ml\")`\n",
    "- Use interactive mode: `report.apply_interactive()`\n",
    "- Check the CLI: `datawash analyze your_data.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
